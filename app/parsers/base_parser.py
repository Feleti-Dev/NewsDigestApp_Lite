import logging
import re
from abc import ABC, abstractmethod
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

from app.configs.config import config
from app.database.db_utils import DatabaseManager
from app.ml.llm_client import llm_client

logger = logging.getLogger(__name__)


class BaseParser(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""

    def __init__(self, source_type: str):
        self.source_type = source_type
        self.db_manager = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∂–µ
        self.max_items_per_channel = config.app.max_news_per_channel
        self.collection_period = timedelta(hours=int(config.app.max_news_time_period))
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∫–ª–∞–º—ã (—Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        self.ad_keywords = [
            # –†—É—Å—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            '—Ä–µ–∫–ª–∞–º–∞', '—Ä–µ–∫–ª–∞–º–Ω—ã–π', '—Ä–µ–∫–ª–∞–º—ã', '—Ä–µ–∫–ª–∞–º—É', '—Ä–µ–∫–ª–∞–º–æ–π',
            # '—Å–ø–æ–Ω—Å–æ—Ä', '—Å–ø–æ–Ω—Å–æ—Ä—Å–∫–∏–π', '—Å–ø–æ–Ω—Å–æ—Ä—Å–∫–∞—è', '—Å–ø–æ–Ω—Å–æ—Ä—Å–∫–æ–µ',
            # '–ø–∞—Ä—Ç–Ω–µ—Ä', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∞—è', '–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–æ–µ',
            # '–≤–∞–∫–∞–Ω—Å–∏—è', '–≤–∞–∫–∞–Ω—Å–∏–∏', '–≤–∞–∫–∞–Ω—Å–∏–π', '—Ä–∞–±–æ—Ç–∞', '—Ä–∞–±–æ—Ç—ã',
            # '–Ω–∞–±–æ—Ä', '–Ω–∞–±–æ—Ä –≤ –∫–æ–º–∞–Ω–¥—É', '–∏—â–µ–º', '—Ç—Ä–µ–±—É–µ—Ç—Å—è',
            # '–∑–∞—Ä–∞–±–æ—Ç–æ–∫', '–∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å', '–∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å',
            # '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑–∞—Ç—å', '–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∑–∞–∫–∞–∑–∞',
            # '–∫—É–ø–∏—Ç—å', '–ø—Ä–æ–¥–∞—Ç—å', '–ø–æ–∫—É–ø–∫–∞', '–ø—Ä–æ–¥–∞–∂–∞',
            # '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '—Å–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ',
            # '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è',
            # '–ø—Ä–æ–º–æ–∫–æ–¥', '–∫—É–ø–æ–Ω', '—Å–∫–∏–¥–æ—á–Ω—ã–π –∫—É–ø–æ–Ω',
            # '–∫—É—Ä—Å', '–æ–±—É—á–µ–Ω–∏–µ', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å',
            # '—Ä–∞—Å—Å—ã–ª–∫–∞', '—Ä–∞—Å—Å—ã–ª–∫–∏', 'email-—Ä–∞—Å—Å—ã–ª–∫–∞',
            # '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ', 'seo', '–∫–æ–Ω—Ç–µ–∫—Å—Ç',
            # '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤—ã', '–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤',
            # '–ø–æ–¥–ø–∏—Å–∫–∞', '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–æ—Ñ–æ—Ä–º–∏—Ç—å –ø–æ–¥–ø–∏—Å–∫—É',
            # '–ª–∏–¥', '–ª–∏–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è', '–ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤',

            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            'advertisement', 'ad',
            # 'promotion', 'sponsored', 'sponsor',
            # 'promo', 'promotional',
            'advert', 'advertising',
            # 'vacancy', 'job', 'jobs',
            # 'hiring', 'career', 'recruitment',
            # 'work with us', 'we are hiring', 'looking for',
            # 'earn', 'earn money', 'make money', 'income',
            # 'order', 'buy', 'purchase', 'sell', 'sale',
            # 'discount', 'offer', 'deal', 'special offer',
            # 'free', 'trial', 'free trial',
            # 'coupon', 'voucher', 'promo code', 'discount code',
            # 'course', 'training', 'workshop', 'masterclass',
            # 'newsletter', 'email newsletter', 'mailing list',
            # 'marketing', 'seo', 'digital marketing',
            # 'review', 'reviews', 'leave a review',
            # 'subscription', 'subscribe', 'sign up',
            # 'lead', 'lead generation', 'client acquisition'
        ]

        # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        self.ad_pattern = re.compile(
            r'\b(' + '|'.join(re.escape(kw) for kw in self.ad_keywords) + r')\b',
            re.IGNORECASE
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∫–ª–∞–º—ã
        self.additional_patterns = [
            re.compile(r'#[–∞-—èa-z]*—Ä–µ–∫–ª–∞–º[–∞-—è]*', re.IGNORECASE),  # –•—ç—à—Ç–µ–≥–∏ —Å —Ä–µ–∫–ª–∞–º–æ–π
            re.compile(r'#[–∞-—èa-z]*ad[–∞-—èa-z]*', re.IGNORECASE),  # –•—ç—à—Ç–µ–≥–∏ —Å ad
            #re.compile(r'#[–∞-—èa-z]*job[–∞-—èa-z]*', re.IGNORECASE),  # –•—ç—à—Ç–µ–≥–∏ —Å job
            #re.compile(r'#[–∞-—èa-z]*–≤–∞–∫–∞–Ω—Å[–∞-—è]*', re.IGNORECASE),  # –•—ç—à—Ç–µ–≥–∏ —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π
            #re.compile(r'\b(–∑–≤–æ–Ω–∏|–ø–æ–∑–≤–æ–Ω–∏|call|contact)\b', re.IGNORECASE),  # –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é
            #re.compile(r'\b(—Ç–µ–ª–µ—Ñ–æ–Ω|phone|—Ç–µ–ª\.?)\s*[+\d\s\-()]+', re.IGNORECASE),  # –¢–µ–ª–µ—Ñ–æ–Ω—ã
            #re.compile(r'\b(email|–ø–æ—á—Ç–∞|e-mail)\s*[:=]?\s*[\w\.-]+@[\w\.-]+', re.IGNORECASE),  # Email
        ]

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é is_advertisement:
    async def is_advertisement(self, news_data: Dict[str, Any]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ—Å—Ç —Ä–µ–∫–ª–∞–º–Ω—ã–º

        Args:
            news_data: –î–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏

        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞
        """
        try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM API –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
            title = news_data.get('title', '')
            text = news_data.get('text', '')

            if not title and not text:
                return False

            result = await llm_client.detect_advertisement(title,text)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if result["is_advertisement"] and result["confidence"] > 0.75:
                logger.info(f"‚ö†Ô∏è  LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ —Ä–µ–∫–ª–∞–º—É (confidence: {result['confidence']:.2f}): {title[:50]}..., {news_data.get('url','')}")
                return True
            return False

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞ —Ä–µ–∫–ª–∞–º—É: {e}")
            return True

    def _init_db_manager(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ë–î (–æ—Ç–ª–æ–∂–µ–Ω–Ω–∞—è)"""
        if not self.db_manager:
            self.db_manager = DatabaseManager()

    @abstractmethod
    async def fetch_channel_news(
        self, channel_url: str, channel_id: str
    ) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞"""
        pass

    @abstractmethod
    def extract_news_data(self, raw_data: Any) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö API"""
        pass

    def normalize_news_data(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–∏ –≤ –µ–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ë–î
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if "url" not in news_data:
            logger.warning(f"–ù–æ–≤–æ—Å—Ç—å –±–µ–∑ URL: {news_data.get('title', 'Unknown')}")
            news_data["url"] = ""

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        pub_date = news_data.get("publication_date")

        # –ï—Å–ª–∏ –¥–∞—Ç–∞ —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è datetime, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
        if isinstance(pub_date, datetime):
            # –ï—Å–ª–∏ —É –¥–∞—Ç—ã –µ—Å—Ç—å timezone, —É–±–∏—Ä–∞–µ–º –µ–≥–æ
            if pub_date.tzinfo is not None:
                pub_date = pub_date.astimezone(timezone.utc).replace(tzinfo=None)
        else:
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É –≤ datetime
            try:
                if isinstance(pub_date, str):
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                    formats = [
                        "%Y-%m-%dT%H:%M:%S%z",  # ISO —Å —Ç–∞–π–º–∑–æ–Ω–æ–π
                        "%Y-%m-%dT%H:%M:%S",  # ISO –±–µ–∑ —Ç–∞–π–º–∑–æ–Ω—ã
                        "%Y-%m-%d %H:%M:%S",
                        "%Y-%m-%d %H:%M:%S.%f",
                        "%d.%m.%Y %H:%M:%S",
                        "%d/%m/%Y %H:%M:%S",
                    ]
                    for fmt in formats:
                        try:
                            pub_date = datetime.strptime(pub_date, fmt)
                            # –ï—Å–ª–∏ –µ—Å—Ç—å timezone, —É–±–∏—Ä–∞–µ–º
                            if pub_date.tzinfo is not None:
                                pub_date = pub_date.astimezone(timezone.utc).replace(
                                    tzinfo=None
                                )
                            break
                        except ValueError:
                            continue
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É {pub_date}: {e}")
                pub_date = datetime.now()

        # –ï—Å–ª–∏ –¥–∞—Ç–∞ –≤—Å—ë –µ—â—ë –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
        if not pub_date:
            pub_date = datetime.now()

        # –ë–∞–∑–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        normalized = {
            "Source": self.source_type,
            "News_URL": news_data.get("url", ""),
            "Headline": news_data.get("title", "")[:150],
            "News_text": news_data.get("text", ""),
            "Publication_date": pub_date,  # –¢–µ–ø–µ—Ä—å —Ç–æ—á–Ω–æ datetime
            "Has_image": news_data.get("has_image", False),
            "Image_URL": news_data.get("image_url"),
            "Interest_score": None,
            "Daily_used": False,
            "Weekly_used": False,
            "Monthly_used": False,
            "Publication_error": False,
            "Note": news_data.get("note", ""),
        }

        logger.debug(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {normalized['Headline'][:50]}...")
        logger.debug(
            f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {normalized['Publication_date']}, —Ç–∏–ø: {type(normalized['Publication_date'])}"
        )

        return normalized

    def is_duplicate(self, news_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ URL"""
        self._init_db_manager()
        existing = self.db_manager.get_news_by_url(news_url)
        return existing is not None

    async def process_channel(
        self, channel_url: str, channel_id: str
    ) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ë–î
        """
        try:
            logger.debug(f"–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å {self.source_type}: {channel_id}")

            # 1. –ü–æ–ª—É—á–∞–µ–º –°–´–†–´–ï –Ω–æ–≤–æ—Å—Ç–∏
            raw_news = await self.fetch_channel_news(channel_url, channel_id)

            if not raw_news:
                # logger.warning(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å {channel_id}")
                return []

            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            normalized_news = []
            cutoff_time = datetime.now() - self.collection_period
            
            for raw_item in raw_news:
                # 2.1. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –°–´–†–û–ì–û —Ñ–æ—Ä–º–∞—Ç–∞
                extracted_data = self.extract_news_data(raw_item)
                if not extracted_data:
                    logger.debug(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—ã—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞: {type(raw_item)}"
                    )
                    continue

                # 2.2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –ë–î
                normalized = self.normalize_news_data(extracted_data)

                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Å–∞–º–∞—è –¥–µ—à–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
                pub_date = normalized["Publication_date"]
                if pub_date < cutoff_time:
                    logger.debug(f"–ù–æ–≤–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è: {pub_date}")
                    continue

                # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç (–≤—Ç–æ—Ä–∞—è –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞)
                if self.is_duplicate(normalized["News_URL"]):
                    logger.debug(f"–î—É–±–ª–∏–∫–∞—Ç –Ω–æ–≤–æ—Å—Ç–∏: {normalized['News_URL']}")
                    continue

                normalized_news.append(normalized)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if len(normalized_news) >= self.max_items_per_channel:
                    break

            # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç, –Ω–µ—Ç —Å–º—ã—Å–ª–∞ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –¥–∞–ª—å—à–µ
            if not len(normalized_news):
                return []

            # 3. –ü–∞–∫–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∫–ª–∞–º—É (—Å–∞–º–∞—è –¥–æ—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - LLM)
            logger.info(f"üîç –ü–∞–∫–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∫–ª–∞–º—É: {len(normalized_news)} –Ω–æ–≤–æ—Å—Ç–µ–π...")

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Ä–µ–∫–ª–∞–º—É
            news_for_ads_check = [
                {
                    "title": news.get("Headline", ""),
                    "text": news.get("News_text", ""),
                    "url": news.get("News_URL", "")
                }
                for news in normalized_news
            ]

            # –ü–∞–∫–µ—Ç–Ω—ã–π –≤—ã–∑–æ–≤ LLM
            ads_results = await llm_client.detect_advertisement(news_for_ads_check)

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∫–ª–∞–º—É
            filtered_news = []
            for i, news in enumerate(normalized_news):
                if i < len(ads_results):
                    if ads_results[i]["is_advertisement"] and ads_results[i]["confidence"] > 0.75:
                        logger.info(
                            f"‚ö†Ô∏è  LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ —Ä–µ–∫–ª–∞–º—É (confidence: {ads_results[i]['confidence']:.2f}): "
                            f"{news.get('Headline', '')[:50]}..."
                        )
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∫–ª–∞–º—É
                filtered_news.append(news)

            normalized_news = filtered_news
            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º—ã: {len(normalized_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")


            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(normalized_news)} –Ω–æ–≤–æ—Å—Ç–µ–π —Å {channel_id}")
            return normalized_news[: self.max_items_per_channel]

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–Ω–∞–ª–∞ {channel_id}: {e}")
            return []

    def save_to_database(self, news_items: List[Dict[str, Any]]) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

        Args:
            news_items: –°–ø–∏—Å–æ–∫ –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–• –Ω–æ–≤–æ—Å—Ç–µ–π (—Å –∫–ª—é—á–∞–º–∏ –¥–ª—è –ë–î)

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        self._init_db_manager()
        saved_count = 0

        for news_data in news_items:
            try:
                # –î–ï–ë–ê–ì: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º
                logger.debug(
                    f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç—å: {news_data.get('Headline', '')[:50]}..."
                )
                logger.debug(
                    f"–¢–∏–ø –¥–∞—Ç—ã: {type(news_data.get('Publication_date'))}, –∑–Ω–∞—á–µ–Ω–∏–µ: {news_data.get('Publication_date')}"
                )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ news_data —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
                required_keys = ["Source", "News_URL", "Headline", "Publication_date"]
                missing_keys = [key for key in required_keys if key not in news_data]

                if missing_keys:
                    logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏: {missing_keys}")
                    logger.error(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(news_data.keys())}")
                    continue

                news_item = self.db_manager.add_news(news_data)
                if news_item:
                    saved_count += 1
                    logger.debug(
                        f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {news_item.ID} - {news_item.Headline[:50]}..."
                    )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                import traceback

                logger.error(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")

        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {self.source_type}: {saved_count}")
        return saved_count


    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        pass


    async def calculate_llm_interest_score(self, news_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é ML –º–æ–¥–µ–ª–∏

        Args:
            news_data: –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏

        Returns:
            –û—Ü–µ–Ω–∫–∞ –æ—Ç 0.0000 –¥–æ 1.0000
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM API –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
            result = await llm_client.calculate_interest_score(news_data, config.app.topic)

            logger.debug(
                f"LLM –æ—Ü–µ–Ω–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ '{result[0].get('Headline', '')[:50]}...': {result[0].get('Interest_score',''):.4f} (–ø—Ä–∏—á–∏–Ω–∞: {result[0]['reason']})")
            return result

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: {e}")
            return news_data

    async def filter_by_interest_threshold(self, news_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –ø–æ—Ä–æ–≥—É –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
        
        Args:
            news_items: —Å–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        threshold = config.app.interest_threshold
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM API –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
            filtered = []
            news_items = await self.calculate_llm_interest_score(news_items)
            for news in news_items:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É
                if news.get('Interest_score', 0) >= threshold:
                    filtered.append(news)

            logger.info(f"LLM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(news_items)} -> {len(filtered)} –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ—Ä–æ–≥: {threshold})")
            return filtered

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return news_items